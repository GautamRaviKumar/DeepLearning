{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">  FIT5215: Deep Learning (2023)</span>\n",
    "***\n",
    "*CE/Lecturer (Clayton):*  **Dr Trung Le** | trunglm@monash.edu <br/>\n",
    "*Lecturer (Malaysia):*  **Dr Lim Chern Hong** | lim.chernhong@monash.edu <br/>  <br/>\n",
    "*Tutor:*  **Mr Tuan Nguyen**  \\[tuan.ng@monash.edu \\] | **Dr Binh Nguyen** \\[binh.nguyen1@monash.edu \\] | **Dr Qiuhong Ke** \\[Qiuhong.Ke@monash.edu  \\] \n",
    "<br/> <br/>\n",
    "Faculty of Information Technology, Monash University, Australia\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">Tutorial 5b: Convolutional Neural Network with MiniVGG</span><span style=\"color:red;  font-size: 18px\">***** (highly important)</span> #\n",
    "**This tutorial applies CNN to a more real-world setting using TensorFlow using the `CIFAR-10` dataset. It will cover the following**:\n",
    "1. ***How to implement a simplified version of a real-world CNN named `MiniVGG` to classify the `CIFAR-10` dataset using TF 2.x.***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGGNet, (aka VGG), was first introduced by Simonyan and Zisserman in their 2014 paper, *Very Deep Learning Convolutional Neural Networks for Large-Scale Image Recognition*. \n",
    "The work was the first to demonstrate that architecture with very small (3×3) filters can be successfully trained for very deep networks (16-19 layers) and obtain state-of-the-art classification on the challenging ImageNet classification challenge. <br/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VGG family of Convolutional Neural Networks can be characterized by two key components:\n",
    "1. *All CONV layers in the network using only **3×3** filters.*\n",
    "2. *Stacking multiple CONV => RELU layer sets (where the number of consecutive CONV => RELU layers normally increases as we go deeper) before applying a POOL operation.* <br/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the size of this architecture which compromises the running time, this tutorial will use `MiniVGG` which is a simplified version of `VGG` including its architecture, implementation, and application to the `CIFAR-10` dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">I. MiniVGG for the CIFAR-10 dataset</span> ##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">I.1. Architecture of MiniVGG </span>\n",
    "The following figure and table show the concrete architecture of `MiniVGG`. As mentioned before, `MiniVGG` only uses $3\\times3$ filters. In addition, in the `MiniVGGNet`, we apply the batch norm technique to regularize the training and allow the network to be trained with a higher learning rate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/MiniVGG_Table.png' align='center' width=300/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">I.2. Implementation of MiniVGG </span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">I.2.1 Import necessary packages and modules </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, models, layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">I.2.2 Download, prepare and preprocess CIFAR-10 dataset </span>\n",
    "\n",
    "The CIFAR-10 dataset contains 60,000 color images in 10 classes, with 6,000 images in each class. The dataset is divided into 50,000 training images and 10,000 testing images. The classes are mutually exclusive and there is no overlap between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full) , (X_test_full, y_test_full) = datasets.cifar10.load_data()\n",
    "X_train_full, X_test_full = X_train_full/255.0, X_test_full/255.0\n",
    "print(X_train_full.shape, y_train_full.shape)\n",
    "print(X_test_full.shape, y_test_full.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_idx = np.random.permutation(X_train_full.shape[0])\n",
    "test_idx = np.random.permutation(X_test_full.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tiny `CIFAR-10` for a demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_valid, n_test = 5000, 5000, 5000 \n",
    "X_train, y_train = X_train_full[train_val_idx][:n_train], y_train_full[train_val_idx][:n_train]\n",
    "X_valid, y_valid = X_train_full[train_val_idx][n_train:n_train+n_valid], y_train_full[train_val_idx][n_train:n_train+n_valid]\n",
    "X_test, y_test = X_test_full[test_idx][:n_test], y_test_full[test_idx][:n_test]\n",
    "y_train = y_train.reshape(-1)\n",
    "y_valid = y_valid.reshape(-1)\n",
    "y_test = y_test.reshape(-1)\n",
    "print('Training set', X_train.shape, y_train.shape)\n",
    "print('Validation set', X_valid.shape, y_valid.shape)\n",
    "print('Test set', X_test.shape, y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">I.2.3 Visualize CIFAR-10 images </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def visualize_data(images, categories, images_per_row = 8):\n",
    "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    n_images = len(images)\n",
    "    n_rows = math.ceil(float(n_images)/images_per_row)\n",
    "    fig = plt.figure(figsize=(1.5*images_per_row, 1.5*n_rows))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    for i in range(n_images):\n",
    "        plt.subplot(n_rows, images_per_row, i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(images[i])\n",
    "        class_index = categories[i]\n",
    "        plt.xlabel(class_names[class_index])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_data(X_train[0:32], y_train[0:32])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">I.2.4 Build up MiniVGG network </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vgg():\n",
    "    vgg_model = models.Sequential()\n",
    "    vgg_model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(32,32,3)))\n",
    "    vgg_model.add(layers.BatchNormalization(momentum=0.9))\n",
    "    vgg_model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu'))\n",
    "    vgg_model.add(layers.BatchNormalization(momentum=0.9))\n",
    "    vgg_model.add(layers.MaxPool2D(pool_size=(2,2)))  #downscale the image size by 2\n",
    "    vgg_model.add(layers.Dropout(rate=0.25))  #deactivate 25% of neurons for each feed-forward\n",
    "    vgg_model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "    vgg_model.add(layers.BatchNormalization(momentum=0.9))\n",
    "    vgg_model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "    vgg_model.add(layers.BatchNormalization(momentum=0.9))\n",
    "    vgg_model.add(layers.MaxPool2D(pool_size=(2,2)))  #downscale the image size by 2\n",
    "    vgg_model.add(layers.Dropout(rate=0.25))  #deactivate 25% of neurons for each feed-forward\n",
    "    vgg_model.add(layers.Flatten())\n",
    "    vgg_model.add(layers.Dense(512, activation='relu'))\n",
    "    vgg_model.add(layers.BatchNormalization(momentum=0.9))\n",
    "    vgg_model.add(layers.Dropout(rate=0.5)) \n",
    "    vgg_model.add(layers.Dense(10, activation='softmax')) #ten classes in CIFAR-10\n",
    "    return vgg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = create_vgg()\n",
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "vgg.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "no_aug_history = vgg.fit(x=X_train, y=y_train, batch_size=32, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can evaluate the performance on the CIFAR-10 test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = vgg.evaluate(X_test,  y_test, verbose=1, batch_size=64)\n",
    "print(\"Test acc is {}\".format(test_acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">I.2.5 Plot training loss and accuracy</span> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(no_aug_history.history['accuracy'], label='train accuracy', color='green', marker=\"o\")\n",
    "ax1.plot(no_aug_history.history['val_accuracy'], label='valid accuracy', color='blue', marker = \"v\")\n",
    "ax2.plot(no_aug_history.history['loss'], label = 'train loss', color='orange', marker=\"o\")\n",
    "ax2.plot(no_aug_history.history['val_loss'], label = 'valid loss', color='red', marker = \"v\")\n",
    "ax1.legend(loc=3)\n",
    "\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Accuracy', color='g')\n",
    "ax2.set_ylabel('Loss', color='b')\n",
    "ax2.legend(loc=4)\n",
    "plt.ylim([0.0, 5])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">II. MiniVGG with data augmentation</span> ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(horizontal_flip=True, width_shift_range=0.1, height_shift_range=0.1)\n",
    "datagen.fit(X_train)\n",
    "\n",
    "it = datagen.flow(X_train, y_train, batch_size=32)\n",
    "batch_images, batch_labels = next(it)\n",
    "\n",
    "visualize_data(batch_images, batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = create_vgg()\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "vgg.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "steps = len(X_train) // 32\n",
    "with_aug_history = vgg.fit(it, epochs=30, steps_per_epoch=steps, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = vgg.evaluate(X_test,  y_test, verbose=1, batch_size=64)\n",
    "print(\"Test acc is {}\".format(test_acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe an improvement when evaluating on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(with_aug_history.history['accuracy'], label='train accuracy', color='green', marker=\"o\")\n",
    "ax1.plot(with_aug_history.history['val_accuracy'], label='valid accuracy', color='blue', marker = \"v\")\n",
    "ax2.plot(with_aug_history.history['loss'], label = 'train loss', color='orange', marker=\"o\")\n",
    "ax2.plot(with_aug_history.history['val_loss'], label = 'valid loss', color='red', marker = \"v\")\n",
    "ax1.legend(loc=3)\n",
    "\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Accuracy', color='g')\n",
    "ax2.set_ylabel('Loss', color='b')\n",
    "ax2.legend(loc=4)\n",
    "plt.ylim([0.0, 5])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">III. Visualize filters and feature maps of MiniVGG</span> ##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the filters and feature maps is important to help us understand what is learned by trained CNNs. We expect that low layers can capture low-level features such as edges, pixels, and corners, middle layers can capture mid-level features such as circles, boxes, triangles, while high layers can capture high-level features such as objects. According to the operational principle of CNNs, features captured by higher layers are a combination of those in lower layers. For example, edges combine into boxes, and boxes and triangles, in turn, combine into more complicated objects.\n",
    "\n",
    "We now explore how to visualize the filters and feature maps for our trained MiniVGG network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first play around with how to get all layers in a TF keras model and how to extract information from a specific layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(vgg.layers):\n",
    "    print(\"Layer {}: {}\".format(i, layer.name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we examine all Conv2D layers in our network and print out their filters. Recall that the set of filters is represented as a $4D$ tensor with the shape $[fil\\_height, fil\\_width, fil\\_depth, num\\_filters]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg.layers:\n",
    "    if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "        print(\"Layer name: {}, shape: [{}, {}]\".format(layer.name, layer.filters, layer.kernel_size))\n",
    "        print(layer.get_weights()[0].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now visualize a filter in a Conv2D layer of a specific model. Note that each filter is a 3D tensor with shape $[fil\\_height, fil\\_width, fil\\_depth]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def visualize_filter(model, layer_name= None, filter_index=1, n_cols = 8):\n",
    "    layer = model.get_layer(layer_name)\n",
    "    assert isinstance(layer, tf.keras.layers.Conv2D), \"The current layer is not a Conv2D layer\"\n",
    "    filters = layer.get_weights()[0]  #4D tensor of all filters\n",
    "    assert filter_index <= filters.shape[-1], \"The filter index is out of range\"\n",
    "    one_filter = filters[:,:,:, filter_index-1]  #the filter we need to visualize\n",
    "    fil_depth = one_filter.shape[-1]\n",
    "    n_rows = math.ceil(fil_depth/n_cols)\n",
    "    fig = plt.figure(figsize=(n_cols*1.5, n_rows*1.5))\n",
    "    plt.axis(\"off\")\n",
    "    for i in range(fil_depth):\n",
    "        plt.subplot(n_rows, n_cols, i+1)\n",
    "        plt.imshow(one_filter[:,:,i], cmap=\"gray\")\n",
    "        plt.xlabel(\"{} of {}\".format(i, filter_index))\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We visualize the 5th filter in the filters of the convolutional layer {} (Layer 2)\".format(vgg.layers[2].name))\n",
    "visualize_filter(vgg, vgg.layers[2].name, filter_index=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We visualize the 5th filter in the filters of the convolutional layer {} (Layer 6)\".format(vgg.layers[6].name))\n",
    "visualize_filter(vgg, vgg.layers[6].name, filter_index=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **<span style=\"color:red\">Exercise 1</span>:** Expand the function `visualize_filter` so that if `filter_index= None`, we visualize all filters of the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_filter_all(model, layer_name=None, n_cols=8):\n",
    "    layer = model.get_layer(layer_name)\n",
    "    assert isinstance(layer, tf.keras.layers.Conv2D), \"The current layer is not a Conv2D layer\"\n",
    "    filters = layer.get_weights()[0]  # 4D tensor of all filters\n",
    "    for filter_index in range(filters.shape[-1]):\n",
    "        visualize_filter(model, layer_name, filter_index, n_cols=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_filter_all(vgg, vgg.layers[2].name)  # Assume we visualize layer 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us visualize the feature maps before and after training via feeding an image to the network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose an arbitrary image from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2, 2))\n",
    "rand_img = X_train[5]\n",
    "plt.imshow(rand_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature_maps(input_img= None, model= None, layer_name= None, n_cols= 8):\n",
    "    intermediate_model = tf.keras.Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "    if len(input_img.shape)==3:\n",
    "        input_img = tf.expand_dims(input_img, axis =0)\n",
    "    intermediate_output = intermediate_model.predict(input_img)\n",
    "    n_feature_maps = intermediate_output.shape[-1]\n",
    "    n_rows = math.ceil(n_feature_maps/n_cols)\n",
    "    fig = plt.figure(figsize=(n_cols*1.5, n_rows*1.5))\n",
    "    plt.axis(\"off\")\n",
    "    for i in range(n_feature_maps):\n",
    "        plt.subplot(n_rows, n_cols, i+1)\n",
    "        plt.imshow(intermediate_output[0,:,:, i], cmap=\"gray\")\n",
    "        plt.xlabel(\"{} of {}\".format(i, n_feature_maps), fontsize=10)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing feature maps before training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_init = create_vgg()\n",
    "print(\"Before the training:\")\n",
    "print(\"Feature maps at layer {}.\".format(vgg_init.layers[2].name))\n",
    "visualize_feature_maps(rand_img, vgg_init, vgg_init.layers[2].name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing feature maps after training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature maps at 0th layer.\")\n",
    "visualize_feature_maps(rand_img, vgg, vgg.layers[0].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature maps at 4th layer.\")\n",
    "visualize_feature_maps(rand_img, vgg, vgg.layers[4].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature maps at 8th layer.\")\n",
    "visualize_feature_maps(rand_img, vgg, vgg.layers[8].name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">Exercise 2</span>:** Choose another arbitrary image and expand the function `visualize_feature_maps` to visualize the original image (as the first image) along with its feature maps as above. The expected results should display (`n_feature_maps` + 1) figures. The caption for the image is its class name (true label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature_maps(input_img=None, true_label=None, model=None, layer_name=None, n_cols=8):\n",
    "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    if len(input_img.shape)==3:\n",
    "        batch_input_img = tf.expand_dims(input_img, axis =0)  #make it a 4D tensor with batch_size=1\n",
    "    intermediate_model = tf.keras.Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "    intermediate_output = intermediate_model.predict(batch_input_img)\n",
    "    n_feature_maps = intermediate_output.shape[-1]\n",
    "    n_images = math.floor(n_feature_maps) + 1\n",
    "    n_rows = math.ceil(n_images/n_cols)\n",
    "    fig = plt.figure(figsize=(n_cols*1.5, n_rows*1.5))\n",
    "    plt.axis(\"off\")\n",
    "    for i in range(n_images):\n",
    "        plt.subplot(n_rows, n_cols, i+1)\n",
    "        img = input_img if i==0 else intermediate_output[0,:,:, i-1]\n",
    "        label = \"{}\".format(class_names[true_label]) if i==0 else \"{} of {}\".format(i, n_feature_maps)\n",
    "        if i==0:\n",
    "            plt.imshow(img)\n",
    "        else:\n",
    "            plt.imshow(img, cmap= \"gray\")\n",
    "        plt.xlabel(label, fontsize= 10)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_feature_maps(X_train[0], y_train[0], vgg, vgg.layers[6].name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">Exercise 3</span>:** In the lecture and tutorial of week 7, we will study how to implement SOTA CNN networks for example RESNET, DenseNet and also how to perform transfer learning and fine-tuning a SOTA pretrained CNN to fit a specific and tiny dataset. It is appealing now to investigate what is learned by a SOTA pretrained CNN by visualizing its filters. You will have more sense of why CNNs work well for visual data. Your task is to apply the above code to visualize the filters of a pretrained SOTA CNN. Please use the following code as a good start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading network...\")\n",
    "base_model = VGG16(weights=\"imagenet\", include_top=False, input_tensor = layers.Input(shape=(32,32,3)))\n",
    "print(\"Showing layers...\")\n",
    "# loop over the layers in the network and display them to the console\n",
    "for (i, layer) in enumerate(base_model.layers):\n",
    "    print(\"Layer {}: {}\".format(i, layer.__class__.__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_model.layers[2].weights[0].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize a filter using `visualize_filter` or visualize all filters of a convolutional layer using `visualize_filter_all` in the solution of Exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_filter(base_model, 'block1_conv2', filter_index=0)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <span style=\"color:#0b486b\"> <div  style=\"text-align:center\">**THE END**</div> </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
